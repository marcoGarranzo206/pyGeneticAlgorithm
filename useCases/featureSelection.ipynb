{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52144718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyGeneticAlgorithm.discrete_solver import discreteGeneticSolver\n",
    "import pandas as pd\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e42e6cd",
   "metadata": {},
   "source": [
    "# Using GA for feature selection\n",
    "\n",
    "Feature selection in machine learning refers to selecting a subset of features to build your machine learning model. The idea is to remove irrelevant or redundant features, ending with a reduced set of features that can: <br>\n",
    "\n",
    "* Improve model performance by alleviating the curse of dimensionality and reducing overfitting\n",
    "* Reduce the cost of data acquisition. In genomics research, we can probe the entire human genome for some biomedical application, such as classifying cancer patients based on their genome. If we can reduce the number of genes probed (from the entirety of the human genome to a few hundred) we can reduce the costs of data acquisition.\n",
    "* Reduce data storage and shorter processing times: even if the data acquisition is the same after feature selection, we can still benefit from feature selection by reducing storage costs and processing power neccessary\n",
    "* Model simplification and better interpretability: I would rather interpret a 10 variable model than a 100 variable model, how about you?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f6b5cb",
   "metadata": {},
   "source": [
    "# Feature selection in the arcene data\n",
    "\n",
    "The arcene dataset, described here, https://www.openml.org/d/1458, is a dataset of mass spectrometry data, where we try to classify cancer and non cancer patients from protein abundance values. I downloaded the csv filed which was called php8tg99.csv and renamed it to arcene_data.csv. There are 200 data points and 10000 variables in the data. We split the data 60% train, 20% validation, 20% test\n",
    "\n",
    "Fitness function: since this is a classification problem, classification accuracy seems  the logical choice. We could also try to optimze for sparseness. We fix the test set ahead of time, and dont use it till the very end, no peaking at the test set!\n",
    "\n",
    "In order to find the feature subset which generalizes best to unseen data, we split the remaining training data into train and validation, and calculate our fitness w.r.t to validation. To avoid fitting the variable selection to one subset of the data, we split training and validation at every model evaluation.\n",
    "\n",
    "We will try the default QDA lassification algorithm. We should of course try to search for hyperparameters and other algorithms in a real setting, but that goes beyond the scope of this notebook.\n",
    "\n",
    "Once we find the best subset, we train it on the whole train set and evaluate it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "82ea99f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./arcene_data.csv\",sep = \",\")\n",
    "X = data.values[:,:-1]\n",
    "Y = data.values[:,-1]\n",
    "\n",
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c07750af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(features):\n",
    "    \n",
    "    features = np.array(features)\n",
    "    #print(features)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_valid, y_train_valid, test_size=0.25)\n",
    "    x_train = X_train[:,features]\n",
    "    x_val = X_val[:,features]\n",
    "    cls = QDA()\n",
    "    cls.fit(x_train, y_train.ravel())\n",
    "\n",
    "    return np.mean(cls.predict(x_val) == y_val.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fdbb6469",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.775\n",
      "0.8\n",
      "0.75\n",
      "0.8\n",
      "0.825\n",
      "0.825\n",
      "0.8\n",
      "0.775\n",
      "0.85\n",
      "0.775\n",
      "0.75\n",
      "0.8\n",
      "0.775\n",
      "0.825\n",
      "0.8\n",
      "0.85\n",
      "0.8\n",
      "0.85\n",
      "0.8\n",
      "0.825\n",
      "0.775\n",
      "0.8\n",
      "0.8\n",
      "0.8\n",
      "0.825\n"
     ]
    }
   ],
   "source": [
    "dGS = discreteGeneticSolver(0.01,\"midpoint\",[False,True],X_train_valid.shape[1],fitness,pop=200)\n",
    "res = np.array(dGS.solve(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "48ac9585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best subset had 4909 features\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best subset had {np.sum(res)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "496bc14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy using GA features: 0.625\n"
     ]
    }
   ],
   "source": [
    "cls = QDA()\n",
    "cls.fit(X_train_valid[:,res], y_train_valid.ravel())\n",
    "test_pred =  cls.predict(X_test[:,res])\n",
    "print(f\"Test accuracy using GA features: {np.mean(test_pred == y_test.ravel())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "049d36d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy using all features: 0.5\n"
     ]
    }
   ],
   "source": [
    "cls = QDA()\n",
    "cls.fit(X_train_valid, y_train_valid.ravel())\n",
    "test_pred =  cls.predict(X_test)\n",
    "print(f\"Test accuracy using all features: {np.mean(test_pred == y_test.ravel())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
